{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: TITAN X (Pascal) (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "#from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from data_loader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            img=img.transpose(2,0,1)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "                \n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "                        \n",
    "                img_A=img_A.transpose(2, 0, 1)\n",
    "                img_B=img_B.transpose(2, 0, 1)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        #print(img.shape)\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
    "\n",
    "\n",
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.channels, self.img_rows, self.img_cols)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'apple2orange'\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (1, patch, patch)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "         #   d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "          #  u = InstanceNormalization()(u)\n",
    "            u = Concatenate(axis=1)([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "           # if normalization:\n",
    "              #  d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        self.g_AB.summary()\n",
    "        self.g_BA.summary()\n",
    "        self.d_A.summary()\n",
    "        self.d_B.summary()\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "               # print(imgs_A.shape)\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "                #print(fake_A.shape)\n",
    "                #print(fake_B.shape)\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i, self.data_loader.n_batches,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "        imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                \n",
    "                axs[i,j].imshow(gen_imgs[cnt].transpose(1,2,0))\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 3, 128, 128)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 64, 64)   1568        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 32, 64, 64)   0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 32, 32)   32832       leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 64, 32, 32)   0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 128, 16, 16)  131200      leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 128, 16, 16)  0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 256, 8, 8)    524544      leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 256, 8, 8)    0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 256, 16, 16)  0           leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 128, 16, 16)  524416      up_sampling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 16, 16)  0           conv2d_41[0][0]                  \n",
      "                                                                 leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 256, 32, 32)  0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 32, 32)   262208      up_sampling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 32, 32)  0           conv2d_42[0][0]                  \n",
      "                                                                 leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 128, 64, 64)  0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 64, 64)   65568       up_sampling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 64, 64, 64)   0           conv2d_43[0][0]                  \n",
      "                                                                 leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 64, 128, 128) 0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 3, 128, 128)  3075        up_sampling2d_12[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,545,411\n",
      "Trainable params: 1,545,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 3, 128, 128)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 64, 64)   1568        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 32, 64, 64)   0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 32, 32)   32832       leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 64, 32, 32)   0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 128, 16, 16)  131200      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 128, 16, 16)  0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 256, 8, 8)    524544      leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 256, 8, 8)    0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 256, 16, 16)  0           leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 128, 16, 16)  524416      up_sampling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256, 16, 16)  0           conv2d_49[0][0]                  \n",
      "                                                                 leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 256, 32, 32)  0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 32, 32)   262208      up_sampling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128, 32, 32)  0           conv2d_50[0][0]                  \n",
      "                                                                 leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 128, 64, 64)  0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 64, 64)   65568       up_sampling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 64, 64, 64)   0           conv2d_51[0][0]                  \n",
      "                                                                 leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling2D) (None, 64, 128, 128) 0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 3, 128, 128)  3075        up_sampling2d_16[0][0]           \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 1,545,411\n",
      "Trainable params: 1,545,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 3, 128, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 64, 64, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 128, 32, 32)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 256, 16, 16)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 256, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 512, 8, 8)         2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 1, 8, 8)           8193      \n",
      "=================================================================\n",
      "Total params: 5,529,474\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 2,764,737\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 3, 128, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 64, 64, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 128, 32, 32)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 128, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 256, 16, 16)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 256, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 512, 8, 8)         2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 512, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 1, 8, 8)           8193      \n",
      "=================================================================\n",
      "Total params: 5,529,474\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 2,764,737\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilybla/miniconda2/envs/py3keras/lib/python3.6/site-packages/ipykernel_launcher.py:78: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/emilybla/miniconda2/envs/py3keras/lib/python3.6/site-packages/ipykernel_launcher.py:52: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/emilybla/miniconda2/envs/py3keras/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 0/995] [D loss: 0.491988, acc:  12%] [G loss: 13.144844, adv: 0.845790, recon: 0.521943, id: 0.481205] time: 0:00:47.824295 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilybla/miniconda2/envs/py3keras/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 1/995] [D loss: 0.201807, acc:   0%] [G loss: 14.387709, adv: 0.556712, recon: 0.603014, id: 0.666763] time: 0:00:48.030190 \n",
      "[Epoch 0/20] [Batch 2/995] [D loss: 0.210049, acc:   0%] [G loss: 13.016171, adv: 0.872835, recon: 0.503889, id: 0.657871] time: 0:00:48.119910 \n",
      "[Epoch 0/20] [Batch 3/995] [D loss: 0.306497, acc:   0%] [G loss: 13.268682, adv: 0.816637, recon: 0.522135, id: 0.762189] time: 0:00:48.206175 \n",
      "[Epoch 0/20] [Batch 4/995] [D loss: 0.462707, acc:   0%] [G loss: 13.883629, adv: 0.835409, recon: 0.545820, id: 0.795690] time: 0:00:48.291168 \n",
      "[Epoch 0/20] [Batch 5/995] [D loss: 0.323736, acc:   3%] [G loss: 9.532618, adv: 0.703014, recon: 0.359627, id: 0.422324] time: 0:00:48.374755 \n",
      "[Epoch 0/20] [Batch 6/995] [D loss: 0.498176, acc:   0%] [G loss: 12.562208, adv: 0.785037, recon: 0.475514, id: 1.113535] time: 0:00:48.548114 \n",
      "[Epoch 0/20] [Batch 7/995] [D loss: 0.364966, acc:   0%] [G loss: 11.086794, adv: 0.913134, recon: 0.401184, id: 0.734826] time: 0:00:48.632962 \n",
      "[Epoch 0/20] [Batch 8/995] [D loss: 0.384477, acc:   0%] [G loss: 7.947121, adv: 0.840979, recon: 0.269300, id: 0.528906] time: 0:00:48.729341 \n",
      "[Epoch 0/20] [Batch 9/995] [D loss: 0.394295, acc:   9%] [G loss: 8.888291, adv: 0.803306, recon: 0.319247, id: 0.493565] time: 0:00:48.811950 \n",
      "[Epoch 0/20] [Batch 10/995] [D loss: 0.319658, acc:   0%] [G loss: 9.789920, adv: 0.696687, recon: 0.355640, id: 0.665826] time: 0:00:48.898043 \n",
      "[Epoch 0/20] [Batch 11/995] [D loss: 0.312017, acc:  18%] [G loss: 11.394140, adv: 0.726970, recon: 0.402499, id: 1.036038] time: 0:00:49.082043 \n",
      "[Epoch 0/20] [Batch 12/995] [D loss: 0.382139, acc:   0%] [G loss: 9.170580, adv: 0.691976, recon: 0.315323, id: 1.043341] time: 0:00:49.171889 \n",
      "[Epoch 0/20] [Batch 13/995] [D loss: 0.407496, acc:  12%] [G loss: 8.630336, adv: 0.627994, recon: 0.305569, id: 0.872109] time: 0:00:49.252052 \n",
      "[Epoch 0/20] [Batch 14/995] [D loss: 0.305289, acc:   0%] [G loss: 7.819415, adv: 0.595174, recon: 0.262744, id: 0.860362] time: 0:00:49.334740 \n",
      "[Epoch 0/20] [Batch 15/995] [D loss: 0.187979, acc:   6%] [G loss: 11.976883, adv: 0.558051, recon: 0.427407, id: 1.227233] time: 0:00:49.418398 \n",
      "[Epoch 0/20] [Batch 16/995] [D loss: 0.376759, acc:   0%] [G loss: 8.356344, adv: 0.608369, recon: 0.269648, id: 1.027120] time: 0:00:49.589720 \n",
      "[Epoch 0/20] [Batch 17/995] [D loss: 0.205346, acc:   0%] [G loss: 8.444163, adv: 0.572516, recon: 0.288816, id: 0.571330] time: 0:00:49.676015 \n",
      "[Epoch 0/20] [Batch 18/995] [D loss: 0.525092, acc:   0%] [G loss: 10.988503, adv: 0.628483, recon: 0.393049, id: 1.314731] time: 0:00:49.759187 \n",
      "[Epoch 0/20] [Batch 19/995] [D loss: 0.274814, acc:  15%] [G loss: 7.396760, adv: 0.660681, recon: 0.220265, id: 1.099463] time: 0:00:49.851263 \n",
      "[Epoch 0/20] [Batch 20/995] [D loss: 0.633051, acc:   0%] [G loss: 10.045831, adv: 0.683660, recon: 0.328755, id: 1.474575] time: 0:00:49.936390 \n",
      "[Epoch 0/20] [Batch 21/995] [D loss: 0.237188, acc:  31%] [G loss: 10.368904, adv: 0.772344, recon: 0.303878, id: 1.720121] time: 0:00:50.111638 \n",
      "[Epoch 0/20] [Batch 22/995] [D loss: 0.349351, acc:  37%] [G loss: 7.278067, adv: 0.840118, recon: 0.208777, id: 1.043178] time: 0:00:50.194457 \n",
      "[Epoch 0/20] [Batch 23/995] [D loss: 0.239540, acc:  18%] [G loss: 7.957259, adv: 0.721621, recon: 0.231286, id: 1.370173] time: 0:00:50.278669 \n",
      "[Epoch 0/20] [Batch 24/995] [D loss: 0.496122, acc:   0%] [G loss: 6.500574, adv: 0.590680, recon: 0.215205, id: 0.583391] time: 0:00:50.366977 \n",
      "[Epoch 0/20] [Batch 25/995] [D loss: 0.098302, acc:  18%] [G loss: 7.859209, adv: 0.826511, recon: 0.211437, id: 1.547298] time: 0:00:50.450653 \n",
      "[Epoch 0/20] [Batch 26/995] [D loss: 0.412400, acc:   6%] [G loss: 7.138736, adv: 0.666941, recon: 0.223782, id: 0.958273] time: 0:00:50.636544 \n",
      "[Epoch 0/20] [Batch 27/995] [D loss: 0.256390, acc:   6%] [G loss: 6.400273, adv: 0.700455, recon: 0.172266, id: 1.098966] time: 0:00:50.759873 \n",
      "[Epoch 0/20] [Batch 28/995] [D loss: 0.307397, acc:   0%] [G loss: 6.386473, adv: 0.455903, recon: 0.182274, id: 1.129000] time: 0:00:50.843104 \n",
      "[Epoch 0/20] [Batch 29/995] [D loss: 0.389501, acc:   0%] [G loss: 7.401302, adv: 0.513907, recon: 0.227047, id: 0.849884] time: 0:00:50.925819 \n",
      "[Epoch 0/20] [Batch 30/995] [D loss: 0.110349, acc:   0%] [G loss: 7.487258, adv: 0.714208, recon: 0.186313, id: 1.335645] time: 0:00:51.018568 \n",
      "[Epoch 0/20] [Batch 31/995] [D loss: 0.444429, acc:  21%] [G loss: 6.450371, adv: 0.447858, recon: 0.202989, id: 1.050741] time: 0:00:51.191276 \n",
      "[Epoch 0/20] [Batch 32/995] [D loss: 0.540530, acc:  31%] [G loss: 8.360393, adv: 0.635849, recon: 0.225381, id: 1.196076] time: 0:00:51.272708 \n",
      "[Epoch 0/20] [Batch 33/995] [D loss: 0.361556, acc:  21%] [G loss: 7.424103, adv: 0.713749, recon: 0.235096, id: 0.747276] time: 0:00:51.357162 \n",
      "[Epoch 0/20] [Batch 34/995] [D loss: 0.449789, acc:  37%] [G loss: 6.971122, adv: 0.694273, recon: 0.201883, id: 1.092128] time: 0:00:51.440375 \n",
      "[Epoch 0/20] [Batch 35/995] [D loss: 0.382465, acc:   0%] [G loss: 6.103799, adv: 0.648784, recon: 0.183657, id: 0.717012] time: 0:00:51.522220 \n",
      "[Epoch 0/20] [Batch 36/995] [D loss: 0.318114, acc:   6%] [G loss: 7.095186, adv: 0.580535, recon: 0.226633, id: 0.665789] time: 0:00:51.697353 \n",
      "[Epoch 0/20] [Batch 37/995] [D loss: 0.304636, acc:   0%] [G loss: 7.376051, adv: 0.513954, recon: 0.238051, id: 0.982958] time: 0:00:51.780561 \n",
      "[Epoch 0/20] [Batch 38/995] [D loss: 0.514465, acc:   0%] [G loss: 5.985420, adv: 0.615017, recon: 0.132191, id: 1.218107] time: 0:00:51.864693 \n",
      "[Epoch 0/20] [Batch 39/995] [D loss: 0.278599, acc:  21%] [G loss: 6.106514, adv: 0.720372, recon: 0.151251, id: 1.230018] time: 0:00:51.947621 \n",
      "[Epoch 0/20] [Batch 40/995] [D loss: 0.290621, acc:  28%] [G loss: 8.005114, adv: 0.671432, recon: 0.219341, id: 1.294701] time: 0:00:52.030529 \n",
      "[Epoch 0/20] [Batch 41/995] [D loss: 0.400072, acc:  37%] [G loss: 6.978172, adv: 0.547051, recon: 0.168446, id: 1.629781] time: 0:00:52.201622 \n",
      "[Epoch 0/20] [Batch 42/995] [D loss: 0.422561, acc:   0%] [G loss: 7.119583, adv: 0.460206, recon: 0.243983, id: 0.491402] time: 0:00:52.284837 \n",
      "[Epoch 0/20] [Batch 43/995] [D loss: 0.428829, acc:   0%] [G loss: 7.290090, adv: 0.576422, recon: 0.228026, id: 0.482977] time: 0:00:52.380981 \n",
      "[Epoch 0/20] [Batch 44/995] [D loss: 0.320268, acc:   3%] [G loss: 5.952570, adv: 0.584738, recon: 0.152541, id: 1.157758] time: 0:00:52.467484 \n",
      "[Epoch 0/20] [Batch 45/995] [D loss: 0.297796, acc:  18%] [G loss: 6.982284, adv: 0.544449, recon: 0.191229, id: 1.400021] time: 0:00:52.551629 \n",
      "[Epoch 0/20] [Batch 46/995] [D loss: 0.363735, acc:  18%] [G loss: 6.665973, adv: 0.460423, recon: 0.197327, id: 0.866570] time: 0:00:52.730332 \n",
      "[Epoch 0/20] [Batch 47/995] [D loss: 0.301094, acc:   0%] [G loss: 5.810135, adv: 0.530226, recon: 0.157936, id: 1.178012] time: 0:00:52.815180 \n",
      "[Epoch 0/20] [Batch 48/995] [D loss: 0.288986, acc:   0%] [G loss: 6.290794, adv: 0.488373, recon: 0.206621, id: 0.844725] time: 0:00:52.898253 \n",
      "[Epoch 0/20] [Batch 49/995] [D loss: 0.280137, acc:   0%] [G loss: 5.715961, adv: 0.574026, recon: 0.145557, id: 1.218538] time: 0:00:52.982128 \n",
      "[Epoch 0/20] [Batch 50/995] [D loss: 0.342545, acc:  15%] [G loss: 6.432021, adv: 0.511697, recon: 0.200669, id: 0.681296] time: 0:00:53.072527 \n",
      "[Epoch 0/20] [Batch 51/995] [D loss: 0.275873, acc:   9%] [G loss: 6.365526, adv: 0.431342, recon: 0.192116, id: 1.127809] time: 0:00:53.256220 \n",
      "[Epoch 0/20] [Batch 52/995] [D loss: 0.231842, acc:   0%] [G loss: 5.278533, adv: 0.475401, recon: 0.165522, id: 0.655928] time: 0:00:53.346869 \n",
      "[Epoch 0/20] [Batch 53/995] [D loss: 0.291020, acc:   3%] [G loss: 5.245582, adv: 0.447166, recon: 0.152723, id: 0.918668] time: 0:00:53.430347 \n",
      "[Epoch 0/20] [Batch 54/995] [D loss: 0.303959, acc:   6%] [G loss: 6.455647, adv: 0.410221, recon: 0.186629, id: 1.115316] time: 0:00:53.516384 \n",
      "[Epoch 0/20] [Batch 55/995] [D loss: 0.269844, acc:  12%] [G loss: 6.519300, adv: 0.475192, recon: 0.196287, id: 1.207548] time: 0:00:53.600055 \n",
      "[Epoch 0/20] [Batch 56/995] [D loss: 0.209245, acc:  37%] [G loss: 5.466526, adv: 0.540315, recon: 0.134379, id: 1.401357] time: 0:00:53.770956 \n",
      "[Epoch 0/20] [Batch 57/995] [D loss: 0.534803, acc:   0%] [G loss: 7.271234, adv: 0.416554, recon: 0.236703, id: 0.702607] time: 0:00:53.854400 \n",
      "[Epoch 0/20] [Batch 58/995] [D loss: 0.282244, acc:  31%] [G loss: 8.047837, adv: 0.505715, recon: 0.271411, id: 0.734458] time: 0:00:53.938051 \n",
      "[Epoch 0/20] [Batch 59/995] [D loss: 0.338646, acc:  25%] [G loss: 5.790443, adv: 0.526806, recon: 0.158883, id: 0.946400] time: 0:00:54.021124 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 60/995] [D loss: 0.386860, acc:  18%] [G loss: 5.895486, adv: 0.471926, recon: 0.163781, id: 1.145600] time: 0:00:54.104288 \n",
      "[Epoch 0/20] [Batch 61/995] [D loss: 0.307775, acc:  28%] [G loss: 6.438945, adv: 0.555931, recon: 0.136261, id: 1.681903] time: 0:00:54.287671 \n",
      "[Epoch 0/20] [Batch 62/995] [D loss: 0.336841, acc:  21%] [G loss: 6.311774, adv: 0.536274, recon: 0.190808, id: 0.981720] time: 0:00:54.374716 \n",
      "[Epoch 0/20] [Batch 63/995] [D loss: 0.232349, acc:  18%] [G loss: 6.803513, adv: 0.522774, recon: 0.233448, id: 0.755218] time: 0:00:54.462193 \n",
      "[Epoch 0/20] [Batch 64/995] [D loss: 0.335243, acc:   3%] [G loss: 6.403176, adv: 0.427376, recon: 0.184544, id: 1.034532] time: 0:00:54.546091 \n",
      "[Epoch 0/20] [Batch 65/995] [D loss: 0.284454, acc:  31%] [G loss: 5.916928, adv: 0.523804, recon: 0.129133, id: 1.800489] time: 0:00:54.639516 \n",
      "[Epoch 0/20] [Batch 66/995] [D loss: 0.199495, acc:  68%] [G loss: 7.994158, adv: 0.589340, recon: 0.239579, id: 0.908292] time: 0:00:54.810992 \n",
      "[Epoch 0/20] [Batch 67/995] [D loss: 0.283100, acc:  34%] [G loss: 6.779357, adv: 0.597944, recon: 0.197562, id: 0.727831] time: 0:00:54.886519 \n",
      "[Epoch 0/20] [Batch 68/995] [D loss: 0.349364, acc:  15%] [G loss: 6.157718, adv: 0.489283, recon: 0.200108, id: 0.808069] time: 0:00:54.969563 \n",
      "[Epoch 0/20] [Batch 69/995] [D loss: 0.265798, acc:  15%] [G loss: 6.204906, adv: 0.434201, recon: 0.180921, id: 1.244017] time: 0:00:55.053024 \n",
      "[Epoch 0/20] [Batch 70/995] [D loss: 0.282670, acc:   9%] [G loss: 5.509644, adv: 0.503402, recon: 0.165546, id: 0.824555] time: 0:00:55.140752 \n",
      "[Epoch 0/20] [Batch 71/995] [D loss: 0.282267, acc:   3%] [G loss: 5.185685, adv: 0.432055, recon: 0.153780, id: 0.691767] time: 0:00:55.325453 \n",
      "[Epoch 0/20] [Batch 72/995] [D loss: 0.390323, acc:  12%] [G loss: 6.823599, adv: 0.539213, recon: 0.202196, id: 1.079065] time: 0:00:55.408909 \n",
      "[Epoch 0/20] [Batch 73/995] [D loss: 0.265146, acc:  12%] [G loss: 6.001318, adv: 0.471651, recon: 0.175017, id: 0.946636] time: 0:00:55.493967 \n",
      "[Epoch 0/20] [Batch 74/995] [D loss: 0.243995, acc:  15%] [G loss: 6.888664, adv: 0.467797, recon: 0.190378, id: 1.277586] time: 0:00:55.586025 \n",
      "[Epoch 0/20] [Batch 75/995] [D loss: 0.261710, acc:  12%] [G loss: 4.302067, adv: 0.457139, recon: 0.105932, id: 0.970613] time: 0:00:55.672960 \n",
      "[Epoch 0/20] [Batch 76/995] [D loss: 0.249577, acc:   0%] [G loss: 5.801123, adv: 0.504671, recon: 0.167592, id: 0.881644] time: 0:00:55.872707 \n",
      "[Epoch 0/20] [Batch 77/995] [D loss: 0.380383, acc:   3%] [G loss: 5.538207, adv: 0.422094, recon: 0.156341, id: 0.824369] time: 0:00:55.956279 \n",
      "[Epoch 0/20] [Batch 78/995] [D loss: 0.259368, acc:  21%] [G loss: 6.719473, adv: 0.491297, recon: 0.217637, id: 0.786336] time: 0:00:56.040943 \n",
      "[Epoch 0/20] [Batch 79/995] [D loss: 0.338676, acc:  18%] [G loss: 4.690846, adv: 0.476226, recon: 0.131260, id: 0.634916] time: 0:00:56.124279 \n",
      "[Epoch 0/20] [Batch 80/995] [D loss: 0.286900, acc:  25%] [G loss: 6.541723, adv: 0.428010, recon: 0.216176, id: 0.719522] time: 0:00:56.210517 \n",
      "[Epoch 0/20] [Batch 81/995] [D loss: 0.305273, acc:  18%] [G loss: 5.217600, adv: 0.437517, recon: 0.135421, id: 0.897909] time: 0:00:56.388703 \n",
      "[Epoch 0/20] [Batch 82/995] [D loss: 0.244847, acc:  28%] [G loss: 5.385396, adv: 0.445983, recon: 0.148010, id: 1.276639] time: 0:00:56.473173 \n",
      "[Epoch 0/20] [Batch 83/995] [D loss: 0.319545, acc:   0%] [G loss: 5.599646, adv: 0.428222, recon: 0.176609, id: 0.547478] time: 0:00:56.556635 \n",
      "[Epoch 0/20] [Batch 84/995] [D loss: 0.283835, acc:  28%] [G loss: 5.849075, adv: 0.461806, recon: 0.163969, id: 1.086593] time: 0:00:56.639798 \n",
      "[Epoch 0/20] [Batch 85/995] [D loss: 0.206450, acc:  40%] [G loss: 6.019489, adv: 0.483889, recon: 0.158648, id: 1.243562] time: 0:00:56.723232 \n",
      "[Epoch 0/20] [Batch 86/995] [D loss: 0.259184, acc:  28%] [G loss: 5.883846, adv: 0.396878, recon: 0.178624, id: 1.049331] time: 0:00:56.894697 \n",
      "[Epoch 0/20] [Batch 87/995] [D loss: 0.172210, acc:  50%] [G loss: 6.644643, adv: 0.534914, recon: 0.165930, id: 1.126783] time: 0:00:56.978085 \n",
      "[Epoch 0/20] [Batch 88/995] [D loss: 0.210696, acc:  15%] [G loss: 6.643325, adv: 0.590915, recon: 0.180153, id: 1.252020] time: 0:00:57.072195 \n",
      "[Epoch 0/20] [Batch 89/995] [D loss: 0.265190, acc:  15%] [G loss: 5.655006, adv: 0.534262, recon: 0.139466, id: 1.535620] time: 0:00:57.150083 \n",
      "[Epoch 0/20] [Batch 90/995] [D loss: 0.362860, acc:  12%] [G loss: 5.321221, adv: 0.359436, recon: 0.136697, id: 1.212223] time: 0:00:57.233790 \n",
      "[Epoch 0/20] [Batch 91/995] [D loss: 0.263521, acc:  12%] [G loss: 6.097503, adv: 0.450457, recon: 0.176094, id: 1.028791] time: 0:00:57.410549 \n",
      "[Epoch 0/20] [Batch 92/995] [D loss: 0.272580, acc:   0%] [G loss: 4.779943, adv: 0.424611, recon: 0.124422, id: 0.728468] time: 0:00:57.495162 \n",
      "[Epoch 0/20] [Batch 93/995] [D loss: 0.196942, acc:  12%] [G loss: 6.710664, adv: 0.483782, recon: 0.199990, id: 0.863195] time: 0:00:57.577370 \n",
      "[Epoch 0/20] [Batch 94/995] [D loss: 0.266032, acc:   9%] [G loss: 4.912404, adv: 0.390654, recon: 0.131147, id: 0.946100] time: 0:00:57.660522 \n",
      "[Epoch 0/20] [Batch 95/995] [D loss: 0.263349, acc:  21%] [G loss: 5.014441, adv: 0.493558, recon: 0.127290, id: 1.039110] time: 0:00:57.748395 \n",
      "[Epoch 0/20] [Batch 96/995] [D loss: 0.296386, acc:  25%] [G loss: 5.229539, adv: 0.417828, recon: 0.142493, id: 0.947029] time: 0:00:57.919270 \n",
      "[Epoch 0/20] [Batch 97/995] [D loss: 0.284137, acc:  18%] [G loss: 6.066239, adv: 0.459199, recon: 0.163592, id: 1.104905] time: 0:00:58.010227 \n",
      "[Epoch 0/20] [Batch 98/995] [D loss: 0.231332, acc:   3%] [G loss: 5.993914, adv: 0.501911, recon: 0.174508, id: 1.009516] time: 0:00:58.093465 \n",
      "[Epoch 0/20] [Batch 99/995] [D loss: 0.239510, acc:  21%] [G loss: 4.574898, adv: 0.459896, recon: 0.110876, id: 1.073334] time: 0:00:58.189108 \n",
      "[Epoch 0/20] [Batch 100/995] [D loss: 0.400477, acc:  15%] [G loss: 5.109694, adv: 0.345185, recon: 0.172735, id: 0.508687] time: 0:00:58.265609 \n",
      "[Epoch 0/20] [Batch 101/995] [D loss: 0.256864, acc:  18%] [G loss: 6.445169, adv: 0.414242, recon: 0.207927, id: 0.652519] time: 0:00:58.438738 \n",
      "[Epoch 0/20] [Batch 102/995] [D loss: 0.269938, acc:  50%] [G loss: 5.992373, adv: 0.499813, recon: 0.163489, id: 0.664897] time: 0:00:58.522010 \n",
      "[Epoch 0/20] [Batch 103/995] [D loss: 0.130511, acc:  28%] [G loss: 6.607934, adv: 0.579896, recon: 0.187079, id: 0.868985] time: 0:00:58.605351 \n",
      "[Epoch 0/20] [Batch 104/995] [D loss: 0.312536, acc:  34%] [G loss: 5.964638, adv: 0.399448, recon: 0.195942, id: 0.608015] time: 0:00:58.692550 \n",
      "[Epoch 0/20] [Batch 105/995] [D loss: 0.436873, acc:  25%] [G loss: 5.674374, adv: 0.446417, recon: 0.127415, id: 1.351486] time: 0:00:58.780141 \n",
      "[Epoch 0/20] [Batch 106/995] [D loss: 0.201208, acc:  34%] [G loss: 6.508257, adv: 0.611473, recon: 0.155864, id: 1.280852] time: 0:00:58.956403 \n",
      "[Epoch 0/20] [Batch 107/995] [D loss: 0.342992, acc:   0%] [G loss: 5.283609, adv: 0.469711, recon: 0.168145, id: 0.721971] time: 0:00:59.039582 \n",
      "[Epoch 0/20] [Batch 108/995] [D loss: 0.139135, acc:   3%] [G loss: 7.154389, adv: 0.543209, recon: 0.197099, id: 0.897328] time: 0:00:59.125730 \n",
      "[Epoch 0/20] [Batch 109/995] [D loss: 0.348321, acc:  12%] [G loss: 6.338773, adv: 0.409952, recon: 0.182806, id: 1.219862] time: 0:00:59.209082 \n",
      "[Epoch 0/20] [Batch 110/995] [D loss: 0.263491, acc:  40%] [G loss: 5.788158, adv: 0.510506, recon: 0.141047, id: 1.192056] time: 0:00:59.298311 \n",
      "[Epoch 0/20] [Batch 111/995] [D loss: 0.250172, acc:  34%] [G loss: 4.875577, adv: 0.489281, recon: 0.113231, id: 1.295938] time: 0:00:59.474015 \n",
      "[Epoch 0/20] [Batch 112/995] [D loss: 0.351525, acc:  18%] [G loss: 5.844225, adv: 0.329654, recon: 0.172076, id: 1.031324] time: 0:00:59.557596 \n",
      "[Epoch 0/20] [Batch 113/995] [D loss: 0.299336, acc:  28%] [G loss: 5.885785, adv: 0.432504, recon: 0.164507, id: 0.950235] time: 0:00:59.642422 \n",
      "[Epoch 0/20] [Batch 114/995] [D loss: 0.322245, acc:  18%] [G loss: 6.133788, adv: 0.471478, recon: 0.166440, id: 1.054237] time: 0:00:59.725726 \n",
      "[Epoch 0/20] [Batch 115/995] [D loss: 0.266047, acc:  50%] [G loss: 5.748341, adv: 0.394787, recon: 0.152700, id: 1.340834] time: 0:00:59.818964 \n",
      "[Epoch 0/20] [Batch 116/995] [D loss: 0.259003, acc:   3%] [G loss: 5.696404, adv: 0.470179, recon: 0.175382, id: 0.753797] time: 0:01:00.007229 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 117/995] [D loss: 0.173146, acc:   9%] [G loss: 5.792801, adv: 0.461316, recon: 0.163788, id: 0.948438] time: 0:01:00.094170 \n",
      "[Epoch 0/20] [Batch 118/995] [D loss: 0.292248, acc:   0%] [G loss: 4.711597, adv: 0.411412, recon: 0.130857, id: 0.676632] time: 0:01:00.177451 \n",
      "[Epoch 0/20] [Batch 119/995] [D loss: 0.242265, acc:   3%] [G loss: 5.584423, adv: 0.424387, recon: 0.173741, id: 0.729277] time: 0:01:00.269999 \n",
      "[Epoch 0/20] [Batch 120/995] [D loss: 0.338165, acc:  34%] [G loss: 5.031750, adv: 0.351388, recon: 0.125850, id: 1.255454] time: 0:01:00.352956 \n",
      "[Epoch 0/20] [Batch 121/995] [D loss: 0.343467, acc:  34%] [G loss: 5.596136, adv: 0.338014, recon: 0.164412, id: 0.870937] time: 0:01:00.529363 \n",
      "[Epoch 0/20] [Batch 122/995] [D loss: 0.168620, acc:  28%] [G loss: 5.628343, adv: 0.505247, recon: 0.135020, id: 1.281852] time: 0:01:00.612960 \n",
      "[Epoch 0/20] [Batch 123/995] [D loss: 0.430121, acc:   9%] [G loss: 4.922571, adv: 0.492443, recon: 0.119651, id: 1.096014] time: 0:01:00.700879 \n",
      "[Epoch 0/20] [Batch 124/995] [D loss: 0.274509, acc:  18%] [G loss: 4.876268, adv: 0.448215, recon: 0.128361, id: 0.962732] time: 0:01:00.837616 \n",
      "[Epoch 0/20] [Batch 125/995] [D loss: 0.439091, acc:  56%] [G loss: 5.873467, adv: 0.559775, recon: 0.153346, id: 0.541750] time: 0:01:00.929130 \n",
      "[Epoch 0/20] [Batch 126/995] [D loss: 0.213872, acc:  43%] [G loss: 5.698647, adv: 0.656180, recon: 0.156728, id: 0.613673] time: 0:01:01.103644 \n",
      "[Epoch 0/20] [Batch 127/995] [D loss: 0.127049, acc:  37%] [G loss: 6.525953, adv: 0.641529, recon: 0.163635, id: 1.112028] time: 0:01:01.197915 \n",
      "[Epoch 0/20] [Batch 128/995] [D loss: 0.150388, acc:   9%] [G loss: 5.349568, adv: 0.648032, recon: 0.136059, id: 0.684540] time: 0:01:01.284380 \n",
      "[Epoch 0/20] [Batch 129/995] [D loss: 0.349016, acc:   9%] [G loss: 4.623091, adv: 0.448009, recon: 0.133259, id: 0.694520] time: 0:01:01.380827 \n",
      "[Epoch 0/20] [Batch 130/995] [D loss: 0.542121, acc:  40%] [G loss: 5.137126, adv: 0.372002, recon: 0.142550, id: 1.120425] time: 0:01:01.462716 \n",
      "[Epoch 0/20] [Batch 131/995] [D loss: 0.318636, acc:   9%] [G loss: 6.064126, adv: 0.540704, recon: 0.198494, id: 0.508909] time: 0:01:01.642727 \n",
      "[Epoch 0/20] [Batch 132/995] [D loss: 0.293433, acc:   9%] [G loss: 6.755433, adv: 0.380193, recon: 0.191341, id: 0.946058] time: 0:01:01.725758 \n",
      "[Epoch 0/20] [Batch 133/995] [D loss: 0.231620, acc:   3%] [G loss: 6.039068, adv: 0.500681, recon: 0.154267, id: 0.706935] time: 0:01:01.810143 \n",
      "[Epoch 0/20] [Batch 134/995] [D loss: 0.129621, acc:  21%] [G loss: 6.191717, adv: 0.484439, recon: 0.129473, id: 1.291754] time: 0:01:01.896303 \n",
      "[Epoch 0/20] [Batch 135/995] [D loss: 0.113275, acc:  25%] [G loss: 7.015985, adv: 0.505845, recon: 0.196074, id: 0.627992] time: 0:01:01.985802 \n",
      "[Epoch 0/20] [Batch 136/995] [D loss: 0.113518, acc:   9%] [G loss: 7.186855, adv: 0.592149, recon: 0.178291, id: 1.208231] time: 0:01:02.155897 \n",
      "[Epoch 0/20] [Batch 137/995] [D loss: 0.192612, acc:  18%] [G loss: 5.590060, adv: 0.536926, recon: 0.166596, id: 0.881654] time: 0:01:02.240695 \n",
      "[Epoch 0/20] [Batch 138/995] [D loss: 0.279691, acc:  25%] [G loss: 6.973646, adv: 0.548860, recon: 0.193313, id: 1.271847] time: 0:01:02.331481 \n",
      "[Epoch 0/20] [Batch 139/995] [D loss: 0.140460, acc:  46%] [G loss: 5.406584, adv: 0.664135, recon: 0.077353, id: 1.346018] time: 0:01:02.417413 \n",
      "[Epoch 0/20] [Batch 140/995] [D loss: 0.260756, acc:  25%] [G loss: 4.866388, adv: 0.448714, recon: 0.111448, id: 1.213574] time: 0:01:02.507382 \n",
      "[Epoch 0/20] [Batch 141/995] [D loss: 0.224051, acc:  28%] [G loss: 4.400692, adv: 0.455524, recon: 0.095977, id: 1.119315] time: 0:01:02.667700 \n",
      "[Epoch 0/20] [Batch 142/995] [D loss: 0.146507, acc:  28%] [G loss: 6.093293, adv: 0.476526, recon: 0.137197, id: 1.366322] time: 0:01:02.752952 \n",
      "[Epoch 0/20] [Batch 143/995] [D loss: 0.295080, acc:  28%] [G loss: 10.290765, adv: 0.436394, recon: 0.385220, id: 0.587304] time: 0:01:02.837790 \n",
      "[Epoch 0/20] [Batch 144/995] [D loss: 0.418661, acc:   9%] [G loss: 6.090514, adv: 0.516220, recon: 0.143339, id: 1.561623] time: 0:01:02.921136 \n",
      "[Epoch 0/20] [Batch 145/995] [D loss: 0.170606, acc:  15%] [G loss: 9.704174, adv: 0.597335, recon: 0.322932, id: 1.597549] time: 0:01:03.010937 \n",
      "[Epoch 0/20] [Batch 146/995] [D loss: 0.368753, acc:   3%] [G loss: 6.466770, adv: 0.463317, recon: 0.192459, id: 0.918294] time: 0:01:03.202156 \n",
      "[Epoch 0/20] [Batch 147/995] [D loss: 0.377953, acc:  15%] [G loss: 5.371793, adv: 0.415067, recon: 0.120562, id: 0.863694] time: 0:01:03.290262 \n",
      "[Epoch 0/20] [Batch 148/995] [D loss: 0.263982, acc:   0%] [G loss: 6.405594, adv: 0.492448, recon: 0.171731, id: 1.270111] time: 0:01:03.373024 \n",
      "[Epoch 0/20] [Batch 149/995] [D loss: 0.300388, acc:  25%] [G loss: 6.081026, adv: 0.422669, recon: 0.129790, id: 1.337687] time: 0:01:03.464635 \n",
      "[Epoch 0/20] [Batch 150/995] [D loss: 0.262395, acc:   0%] [G loss: 4.797650, adv: 0.415061, recon: 0.128050, id: 1.028507] time: 0:01:03.548030 \n",
      "[Epoch 0/20] [Batch 151/995] [D loss: 0.288846, acc:   0%] [G loss: 5.301418, adv: 0.415944, recon: 0.167365, id: 0.714577] time: 0:01:03.740830 \n",
      "[Epoch 0/20] [Batch 152/995] [D loss: 0.276189, acc:   0%] [G loss: 5.036452, adv: 0.524489, recon: 0.153121, id: 0.542592] time: 0:01:03.825632 \n",
      "[Epoch 0/20] [Batch 153/995] [D loss: 0.285331, acc:   0%] [G loss: 5.804706, adv: 0.446452, recon: 0.168374, id: 0.693728] time: 0:01:03.910795 \n",
      "[Epoch 0/20] [Batch 154/995] [D loss: 0.219082, acc:   0%] [G loss: 5.692341, adv: 0.423063, recon: 0.149131, id: 1.265638] time: 0:01:03.998481 \n",
      "[Epoch 0/20] [Batch 155/995] [D loss: 0.325768, acc:   3%] [G loss: 5.079570, adv: 0.423653, recon: 0.136121, id: 0.601375] time: 0:01:04.082549 \n",
      "[Epoch 0/20] [Batch 156/995] [D loss: 0.258406, acc:   6%] [G loss: 4.330887, adv: 0.430729, recon: 0.127464, id: 0.517400] time: 0:01:04.262385 \n",
      "[Epoch 0/20] [Batch 157/995] [D loss: 0.260069, acc:  15%] [G loss: 5.234353, adv: 0.338272, recon: 0.148958, id: 1.066377] time: 0:01:04.351763 \n",
      "[Epoch 0/20] [Batch 158/995] [D loss: 0.266045, acc:  18%] [G loss: 5.144206, adv: 0.366859, recon: 0.123103, id: 1.019576] time: 0:01:04.436329 \n",
      "[Epoch 0/20] [Batch 159/995] [D loss: 0.275124, acc:  12%] [G loss: 5.248707, adv: 0.528007, recon: 0.156042, id: 0.740891] time: 0:01:04.520165 \n",
      "[Epoch 0/20] [Batch 160/995] [D loss: 0.143845, acc:  25%] [G loss: 4.852512, adv: 0.446504, recon: 0.125743, id: 1.256718] time: 0:01:04.611207 \n",
      "[Epoch 0/20] [Batch 161/995] [D loss: 0.395827, acc:   9%] [G loss: 4.784417, adv: 0.306887, recon: 0.121317, id: 0.924932] time: 0:01:04.778872 \n",
      "[Epoch 0/20] [Batch 162/995] [D loss: 0.290732, acc:  18%] [G loss: 5.541956, adv: 0.345273, recon: 0.151305, id: 1.202585] time: 0:01:04.863871 \n",
      "[Epoch 0/20] [Batch 163/995] [D loss: 0.181950, acc:  21%] [G loss: 5.454104, adv: 0.459761, recon: 0.135709, id: 0.911799] time: 0:01:04.947093 \n",
      "[Epoch 0/20] [Batch 164/995] [D loss: 0.377444, acc:   0%] [G loss: 6.077472, adv: 0.343333, recon: 0.180947, id: 1.234549] time: 0:01:05.031108 \n",
      "[Epoch 0/20] [Batch 165/995] [D loss: 0.213547, acc:  34%] [G loss: 4.675306, adv: 0.439411, recon: 0.119393, id: 1.007649] time: 0:01:05.116697 \n",
      "[Epoch 0/20] [Batch 166/995] [D loss: 0.235022, acc:  25%] [G loss: 5.177611, adv: 0.410770, recon: 0.144847, id: 0.979440] time: 0:01:05.297935 \n",
      "[Epoch 0/20] [Batch 167/995] [D loss: 0.272888, acc:   3%] [G loss: 5.150371, adv: 0.383028, recon: 0.152005, id: 0.772955] time: 0:01:05.381680 \n",
      "[Epoch 0/20] [Batch 168/995] [D loss: 0.322151, acc:  21%] [G loss: 5.141830, adv: 0.349304, recon: 0.138018, id: 0.809024] time: 0:01:05.471317 \n",
      "[Epoch 0/20] [Batch 169/995] [D loss: 0.261230, acc:   3%] [G loss: 6.506485, adv: 0.336964, recon: 0.204523, id: 0.902828] time: 0:01:05.547423 \n",
      "[Epoch 0/20] [Batch 170/995] [D loss: 0.209962, acc:  15%] [G loss: 4.907952, adv: 0.459376, recon: 0.107347, id: 1.309626] time: 0:01:05.631766 \n",
      "[Epoch 0/20] [Batch 171/995] [D loss: 0.300862, acc:  31%] [G loss: 4.333430, adv: 0.353628, recon: 0.123412, id: 0.736689] time: 0:01:05.805723 \n",
      "[Epoch 0/20] [Batch 172/995] [D loss: 0.190190, acc:  15%] [G loss: 5.381269, adv: 0.447906, recon: 0.126817, id: 1.043430] time: 0:01:05.921839 \n",
      "[Epoch 0/20] [Batch 173/995] [D loss: 0.561173, acc:  18%] [G loss: 6.955422, adv: 0.541502, recon: 0.192498, id: 0.783732] time: 0:01:06.004477 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 174/995] [D loss: 0.278575, acc:  40%] [G loss: 5.430655, adv: 0.618287, recon: 0.147163, id: 0.629267] time: 0:01:06.100811 \n",
      "[Epoch 0/20] [Batch 175/995] [D loss: 0.346304, acc:  40%] [G loss: 5.091297, adv: 0.438225, recon: 0.145103, id: 0.843279] time: 0:01:06.193180 \n",
      "[Epoch 0/20] [Batch 176/995] [D loss: 0.280026, acc:  68%] [G loss: 5.698108, adv: 0.556920, recon: 0.127600, id: 0.808496] time: 0:01:06.375129 \n",
      "[Epoch 0/20] [Batch 177/995] [D loss: 0.317845, acc:  46%] [G loss: 5.277506, adv: 0.493715, recon: 0.151866, id: 0.726592] time: 0:01:06.459324 \n",
      "[Epoch 0/20] [Batch 178/995] [D loss: 0.302938, acc:  15%] [G loss: 4.861232, adv: 0.432028, recon: 0.133325, id: 1.003439] time: 0:01:06.542866 \n",
      "[Epoch 0/20] [Batch 179/995] [D loss: 0.321411, acc:   0%] [G loss: 5.630019, adv: 0.391138, recon: 0.145075, id: 1.226325] time: 0:01:06.630448 \n",
      "[Epoch 0/20] [Batch 180/995] [D loss: 0.223599, acc:   0%] [G loss: 5.369964, adv: 0.424435, recon: 0.148566, id: 1.143613] time: 0:01:06.714129 \n",
      "[Epoch 0/20] [Batch 181/995] [D loss: 0.238529, acc:   3%] [G loss: 4.309141, adv: 0.388740, recon: 0.114636, id: 0.825495] time: 0:01:06.886521 \n",
      "[Epoch 0/20] [Batch 182/995] [D loss: 0.302366, acc:   0%] [G loss: 4.462670, adv: 0.472527, recon: 0.111394, id: 0.764164] time: 0:01:06.969952 \n",
      "[Epoch 0/20] [Batch 183/995] [D loss: 0.348697, acc:  34%] [G loss: 5.507279, adv: 0.444648, recon: 0.141019, id: 0.775074] time: 0:01:07.054158 \n",
      "[Epoch 0/20] [Batch 184/995] [D loss: 0.323837, acc:   9%] [G loss: 5.660718, adv: 0.478500, recon: 0.142550, id: 1.314292] time: 0:01:07.138679 \n",
      "[Epoch 0/20] [Batch 185/995] [D loss: 0.242425, acc:  15%] [G loss: 5.023351, adv: 0.475687, recon: 0.149605, id: 0.775822] time: 0:01:07.227686 \n",
      "[Epoch 0/20] [Batch 186/995] [D loss: 0.305973, acc:  18%] [G loss: 5.196253, adv: 0.459180, recon: 0.128600, id: 0.736530] time: 0:01:07.408678 \n",
      "[Epoch 0/20] [Batch 187/995] [D loss: 0.304418, acc:  15%] [G loss: 5.847816, adv: 0.376807, recon: 0.168860, id: 1.064860] time: 0:01:07.495459 \n",
      "[Epoch 0/20] [Batch 188/995] [D loss: 0.393721, acc:  25%] [G loss: 4.385634, adv: 0.297933, recon: 0.120820, id: 0.591310] time: 0:01:07.579869 \n",
      "[Epoch 0/20] [Batch 189/995] [D loss: 0.265057, acc:  18%] [G loss: 3.887467, adv: 0.424535, recon: 0.104732, id: 0.628758] time: 0:01:07.664668 \n",
      "[Epoch 0/20] [Batch 190/995] [D loss: 0.222270, acc:   3%] [G loss: 5.802944, adv: 0.619755, recon: 0.169512, id: 0.886552] time: 0:01:07.750148 \n",
      "[Epoch 0/20] [Batch 191/995] [D loss: 0.222482, acc:   6%] [G loss: 5.104423, adv: 0.484663, recon: 0.132963, id: 0.968698] time: 0:01:07.938268 \n",
      "[Epoch 0/20] [Batch 192/995] [D loss: 0.319269, acc:   0%] [G loss: 5.752884, adv: 0.339094, recon: 0.159947, id: 1.465886] time: 0:01:08.017891 \n",
      "[Epoch 0/20] [Batch 193/995] [D loss: 0.204835, acc:  21%] [G loss: 4.343326, adv: 0.455658, recon: 0.096965, id: 1.112841] time: 0:01:08.113334 \n",
      "[Epoch 0/20] [Batch 194/995] [D loss: 0.320811, acc:  40%] [G loss: 4.377031, adv: 0.353910, recon: 0.137481, id: 0.547170] time: 0:01:08.202017 \n",
      "[Epoch 0/20] [Batch 195/995] [D loss: 0.244467, acc:  15%] [G loss: 5.788935, adv: 0.473417, recon: 0.133208, id: 0.981542] time: 0:01:08.296197 \n",
      "[Epoch 0/20] [Batch 196/995] [D loss: 0.222297, acc:   3%] [G loss: 5.335885, adv: 0.507772, recon: 0.147363, id: 0.822276] time: 0:01:08.477722 \n",
      "[Epoch 0/20] [Batch 197/995] [D loss: 0.363082, acc:  25%] [G loss: 6.694461, adv: 0.423429, recon: 0.138529, id: 1.712099] time: 0:01:08.564388 \n",
      "[Epoch 0/20] [Batch 198/995] [D loss: 0.541316, acc:   6%] [G loss: 6.423664, adv: 0.455121, recon: 0.172822, id: 0.885305] time: 0:01:08.650701 \n",
      "[Epoch 0/20] [Batch 199/995] [D loss: 0.310752, acc:  18%] [G loss: 6.045598, adv: 0.492125, recon: 0.141629, id: 0.885490] time: 0:01:08.737980 \n",
      "[Epoch 0/20] [Batch 200/995] [D loss: 0.307477, acc:  40%] [G loss: 5.862916, adv: 0.507571, recon: 0.125029, id: 1.046275] time: 0:01:08.822762 \n",
      "[Epoch 0/20] [Batch 201/995] [D loss: 0.325522, acc:  18%] [G loss: 5.088503, adv: 0.486941, recon: 0.153536, id: 0.424987] time: 0:01:09.020174 \n",
      "[Epoch 0/20] [Batch 202/995] [D loss: 0.322224, acc:   0%] [G loss: 5.568623, adv: 0.374108, recon: 0.132391, id: 1.666189] time: 0:01:09.105151 \n",
      "[Epoch 0/20] [Batch 203/995] [D loss: 0.300246, acc:   9%] [G loss: 4.799438, adv: 0.389883, recon: 0.135118, id: 0.612827] time: 0:01:09.210641 \n",
      "[Epoch 0/20] [Batch 204/995] [D loss: 0.284641, acc:   0%] [G loss: 4.646135, adv: 0.313060, recon: 0.131262, id: 0.805958] time: 0:01:09.297924 \n",
      "[Epoch 0/20] [Batch 205/995] [D loss: 0.294714, acc:   0%] [G loss: 4.990760, adv: 0.370177, recon: 0.109977, id: 0.846240] time: 0:01:09.386809 \n",
      "[Epoch 0/20] [Batch 206/995] [D loss: 0.300790, acc:  31%] [G loss: 4.844967, adv: 0.387735, recon: 0.122668, id: 0.893944] time: 0:01:09.597048 \n",
      "[Epoch 0/20] [Batch 207/995] [D loss: 0.141098, acc:  34%] [G loss: 5.951157, adv: 0.459690, recon: 0.205041, id: 0.589474] time: 0:01:09.689969 \n",
      "[Epoch 0/20] [Batch 208/995] [D loss: 0.274165, acc:  50%] [G loss: 5.108987, adv: 0.411696, recon: 0.110646, id: 0.962909] time: 0:01:09.790068 \n",
      "[Epoch 0/20] [Batch 209/995] [D loss: 0.269415, acc:  56%] [G loss: 5.848832, adv: 0.434141, recon: 0.142386, id: 1.017584] time: 0:01:09.873689 \n",
      "[Epoch 0/20] [Batch 210/995] [D loss: 0.251431, acc:  46%] [G loss: 5.680113, adv: 0.457262, recon: 0.159000, id: 0.976189] time: 0:01:09.957541 \n",
      "[Epoch 0/20] [Batch 211/995] [D loss: 0.189435, acc:  37%] [G loss: 6.583391, adv: 0.505976, recon: 0.172784, id: 1.378503] time: 0:01:10.142880 \n",
      "[Epoch 0/20] [Batch 212/995] [D loss: 0.348086, acc:  12%] [G loss: 5.020370, adv: 0.299718, recon: 0.150705, id: 0.548968] time: 0:01:10.235810 \n",
      "[Epoch 0/20] [Batch 213/995] [D loss: 0.320830, acc:  12%] [G loss: 5.716413, adv: 0.349750, recon: 0.163005, id: 1.043671] time: 0:01:10.323545 \n",
      "[Epoch 0/20] [Batch 214/995] [D loss: 0.280856, acc:  21%] [G loss: 5.602690, adv: 0.430411, recon: 0.126815, id: 1.330007] time: 0:01:10.414231 \n",
      "[Epoch 0/20] [Batch 215/995] [D loss: 0.249461, acc:  12%] [G loss: 4.806592, adv: 0.434248, recon: 0.118970, id: 1.321758] time: 0:01:10.513765 \n",
      "[Epoch 0/20] [Batch 216/995] [D loss: 0.196034, acc:  21%] [G loss: 5.823378, adv: 0.534760, recon: 0.128057, id: 1.326984] time: 0:01:10.711928 \n",
      "[Epoch 0/20] [Batch 217/995] [D loss: 0.373061, acc:   9%] [G loss: 5.020657, adv: 0.290869, recon: 0.142624, id: 1.039190] time: 0:01:10.803884 \n",
      "[Epoch 0/20] [Batch 218/995] [D loss: 0.296708, acc:   9%] [G loss: 5.170055, adv: 0.385891, recon: 0.140045, id: 0.960782] time: 0:01:10.894609 \n",
      "[Epoch 0/20] [Batch 219/995] [D loss: 0.305812, acc:  21%] [G loss: 4.350047, adv: 0.361313, recon: 0.085468, id: 1.281614] time: 0:01:10.976785 \n",
      "[Epoch 0/20] [Batch 220/995] [D loss: 0.226820, acc:   0%] [G loss: 5.117679, adv: 0.359328, recon: 0.132661, id: 1.216658] time: 0:01:11.128950 \n",
      "[Epoch 0/20] [Batch 221/995] [D loss: 0.184414, acc:   3%] [G loss: 6.546845, adv: 0.322715, recon: 0.196846, id: 0.962092] time: 0:01:11.311418 \n",
      "[Epoch 0/20] [Batch 222/995] [D loss: 0.291415, acc:  15%] [G loss: 5.161354, adv: 0.386073, recon: 0.140652, id: 0.936161] time: 0:01:11.410097 \n",
      "[Epoch 0/20] [Batch 223/995] [D loss: 0.241745, acc:   6%] [G loss: 5.348560, adv: 0.332946, recon: 0.153842, id: 0.912998] time: 0:01:11.496296 \n",
      "[Epoch 0/20] [Batch 224/995] [D loss: 0.169580, acc:   9%] [G loss: 5.336036, adv: 0.435165, recon: 0.143807, id: 1.023536] time: 0:01:11.584403 \n",
      "[Epoch 0/20] [Batch 225/995] [D loss: 0.284331, acc:  28%] [G loss: 4.346336, adv: 0.377855, recon: 0.116567, id: 0.953896] time: 0:01:11.677265 \n",
      "[Epoch 0/20] [Batch 226/995] [D loss: 0.361872, acc:  34%] [G loss: 5.752103, adv: 0.292623, recon: 0.140012, id: 1.044755] time: 0:01:11.881802 \n",
      "[Epoch 0/20] [Batch 227/995] [D loss: 0.249817, acc:  40%] [G loss: 4.596011, adv: 0.367351, recon: 0.109286, id: 1.208821] time: 0:01:11.973680 \n",
      "[Epoch 0/20] [Batch 228/995] [D loss: 0.181606, acc:  15%] [G loss: 5.595674, adv: 0.364003, recon: 0.153412, id: 0.980849] time: 0:01:12.073711 \n",
      "[Epoch 0/20] [Batch 229/995] [D loss: 0.381710, acc:  21%] [G loss: 4.997315, adv: 0.311903, recon: 0.114861, id: 1.025544] time: 0:01:12.177472 \n",
      "[Epoch 0/20] [Batch 230/995] [D loss: 0.205935, acc:  40%] [G loss: 5.630390, adv: 0.437087, recon: 0.142714, id: 1.088783] time: 0:01:12.280606 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 231/995] [D loss: 0.277272, acc:  31%] [G loss: 4.976041, adv: 0.449962, recon: 0.119817, id: 0.872456] time: 0:01:12.484115 \n",
      "[Epoch 0/20] [Batch 232/995] [D loss: 0.329484, acc:  59%] [G loss: 5.251720, adv: 0.368301, recon: 0.143165, id: 0.613560] time: 0:01:12.571230 \n",
      "[Epoch 0/20] [Batch 233/995] [D loss: 0.294035, acc:  46%] [G loss: 4.445196, adv: 0.386452, recon: 0.085674, id: 1.109814] time: 0:01:12.665290 \n",
      "[Epoch 0/20] [Batch 234/995] [D loss: 0.269209, acc:  34%] [G loss: 4.893779, adv: 0.408886, recon: 0.134749, id: 0.910583] time: 0:01:12.752549 \n",
      "[Epoch 0/20] [Batch 235/995] [D loss: 0.248414, acc:  34%] [G loss: 5.768863, adv: 0.377159, recon: 0.128561, id: 1.310859] time: 0:01:12.851505 \n",
      "[Epoch 0/20] [Batch 236/995] [D loss: 0.261475, acc:   0%] [G loss: 5.322168, adv: 0.402790, recon: 0.121187, id: 1.222763] time: 0:01:13.048701 \n",
      "[Epoch 0/20] [Batch 237/995] [D loss: 0.261950, acc:   0%] [G loss: 5.196350, adv: 0.462223, recon: 0.150260, id: 0.794525] time: 0:01:13.143202 \n",
      "[Epoch 0/20] [Batch 238/995] [D loss: 0.343447, acc:  18%] [G loss: 5.351597, adv: 0.397005, recon: 0.154617, id: 0.423915] time: 0:01:13.236242 \n",
      "[Epoch 0/20] [Batch 239/995] [D loss: 0.221602, acc:  31%] [G loss: 4.774960, adv: 0.454581, recon: 0.126094, id: 0.761404] time: 0:01:13.325555 \n",
      "[Epoch 0/20] [Batch 240/995] [D loss: 0.270917, acc:  40%] [G loss: 5.958548, adv: 0.365438, recon: 0.170990, id: 0.763119] time: 0:01:13.415293 \n",
      "[Epoch 0/20] [Batch 241/995] [D loss: 0.205693, acc:  37%] [G loss: 6.374427, adv: 0.486834, recon: 0.166968, id: 1.047863] time: 0:01:13.603690 \n",
      "[Epoch 0/20] [Batch 242/995] [D loss: 0.149022, acc:  31%] [G loss: 5.956247, adv: 0.595238, recon: 0.138931, id: 1.237258] time: 0:01:13.695363 \n",
      "[Epoch 0/20] [Batch 243/995] [D loss: 0.425622, acc:  37%] [G loss: 4.672456, adv: 0.362697, recon: 0.104441, id: 1.012205] time: 0:01:13.782467 \n",
      "[Epoch 0/20] [Batch 244/995] [D loss: 0.299897, acc:  18%] [G loss: 6.681885, adv: 0.371664, recon: 0.189631, id: 1.225295] time: 0:01:13.868840 \n",
      "[Epoch 0/20] [Batch 245/995] [D loss: 0.224442, acc:  31%] [G loss: 4.496898, adv: 0.463659, recon: 0.102470, id: 1.239374] time: 0:01:13.966044 \n",
      "[Epoch 0/20] [Batch 246/995] [D loss: 0.327305, acc:   9%] [G loss: 5.595105, adv: 0.402282, recon: 0.142612, id: 0.969255] time: 0:01:14.168318 \n",
      "[Epoch 0/20] [Batch 247/995] [D loss: 0.293041, acc:  18%] [G loss: 5.423559, adv: 0.335036, recon: 0.153778, id: 1.022482] time: 0:01:14.260077 \n",
      "[Epoch 0/20] [Batch 248/995] [D loss: 0.340673, acc:  15%] [G loss: 4.537379, adv: 0.453724, recon: 0.108666, id: 0.919041] time: 0:01:14.349762 \n",
      "[Epoch 0/20] [Batch 249/995] [D loss: 0.290246, acc:  37%] [G loss: 4.978669, adv: 0.434747, recon: 0.134634, id: 0.957918] time: 0:01:14.451118 \n",
      "[Epoch 0/20] [Batch 250/995] [D loss: 0.256797, acc:  18%] [G loss: 4.667427, adv: 0.347068, recon: 0.122925, id: 1.095590] time: 0:01:14.540247 \n",
      "[Epoch 0/20] [Batch 251/995] [D loss: 0.289420, acc:   3%] [G loss: 7.343113, adv: 0.349022, recon: 0.221289, id: 0.949278] time: 0:01:14.740941 \n",
      "[Epoch 0/20] [Batch 252/995] [D loss: 0.391712, acc:  37%] [G loss: 5.622040, adv: 0.351807, recon: 0.126022, id: 1.124278] time: 0:01:14.835879 \n",
      "[Epoch 0/20] [Batch 253/995] [D loss: 0.221798, acc:  25%] [G loss: 5.188564, adv: 0.509582, recon: 0.118935, id: 1.352434] time: 0:01:14.931381 \n",
      "[Epoch 0/20] [Batch 254/995] [D loss: 0.245688, acc:  34%] [G loss: 6.006817, adv: 0.419271, recon: 0.150105, id: 1.201996] time: 0:01:15.015958 \n",
      "[Epoch 0/20] [Batch 255/995] [D loss: 0.229574, acc:   6%] [G loss: 5.063874, adv: 0.451917, recon: 0.132425, id: 0.682522] time: 0:01:15.106186 \n",
      "[Epoch 0/20] [Batch 256/995] [D loss: 0.250557, acc:  34%] [G loss: 5.870741, adv: 0.393254, recon: 0.116690, id: 1.390454] time: 0:01:15.308750 \n",
      "[Epoch 0/20] [Batch 257/995] [D loss: 0.389232, acc:  31%] [G loss: 5.652814, adv: 0.320046, recon: 0.115133, id: 1.680124] time: 0:01:15.399330 \n",
      "[Epoch 0/20] [Batch 258/995] [D loss: 0.275190, acc:  31%] [G loss: 5.659097, adv: 0.363874, recon: 0.134193, id: 1.432296] time: 0:01:15.485745 \n",
      "[Epoch 0/20] [Batch 259/995] [D loss: 0.263546, acc:  31%] [G loss: 4.741312, adv: 0.375443, recon: 0.119779, id: 0.911368] time: 0:01:15.583782 \n",
      "[Epoch 0/20] [Batch 260/995] [D loss: 0.299184, acc:  18%] [G loss: 5.205175, adv: 0.359598, recon: 0.143648, id: 0.988380] time: 0:01:15.676104 \n",
      "[Epoch 0/20] [Batch 261/995] [D loss: 0.260259, acc:  28%] [G loss: 5.924431, adv: 0.414668, recon: 0.166856, id: 0.757308] time: 0:01:15.873683 \n",
      "[Epoch 0/20] [Batch 262/995] [D loss: 0.231920, acc:   9%] [G loss: 5.669174, adv: 0.438119, recon: 0.164595, id: 0.833158] time: 0:01:15.959890 \n",
      "[Epoch 0/20] [Batch 263/995] [D loss: 0.308503, acc:   3%] [G loss: 4.867361, adv: 0.340442, recon: 0.139626, id: 0.692202] time: 0:01:16.060609 \n",
      "[Epoch 0/20] [Batch 264/995] [D loss: 0.293926, acc:   6%] [G loss: 6.398173, adv: 0.313058, recon: 0.167765, id: 1.394525] time: 0:01:16.164011 \n",
      "[Epoch 0/20] [Batch 265/995] [D loss: 0.262358, acc:   6%] [G loss: 4.989840, adv: 0.452580, recon: 0.120712, id: 0.800154] time: 0:01:16.273528 \n",
      "[Epoch 0/20] [Batch 266/995] [D loss: 0.243228, acc:  21%] [G loss: 5.689806, adv: 0.377875, recon: 0.150601, id: 1.230672] time: 0:01:16.433484 \n",
      "[Epoch 0/20] [Batch 267/995] [D loss: 0.176264, acc:  15%] [G loss: 4.931181, adv: 0.361173, recon: 0.113926, id: 1.335346] time: 0:01:16.517419 \n",
      "[Epoch 0/20] [Batch 268/995] [D loss: 0.205360, acc:   0%] [G loss: 4.828214, adv: 0.412703, recon: 0.103518, id: 1.373212] time: 0:01:16.601161 \n",
      "[Epoch 0/20] [Batch 269/995] [D loss: 0.223601, acc:   3%] [G loss: 6.257665, adv: 0.358129, recon: 0.172394, id: 1.222607] time: 0:01:16.684859 \n",
      "[Epoch 0/20] [Batch 270/995] [D loss: 0.159819, acc:  18%] [G loss: 5.412079, adv: 0.410258, recon: 0.151565, id: 1.050923] time: 0:01:16.774820 \n",
      "[Epoch 0/20] [Batch 271/995] [D loss: 0.281528, acc:  25%] [G loss: 5.293878, adv: 0.357743, recon: 0.143901, id: 1.258403] time: 0:01:16.961981 \n",
      "[Epoch 0/20] [Batch 272/995] [D loss: 0.263853, acc:  25%] [G loss: 5.214837, adv: 0.413729, recon: 0.123666, id: 0.987611] time: 0:01:17.040879 \n",
      "[Epoch 0/20] [Batch 273/995] [D loss: 0.304357, acc:  18%] [G loss: 4.910199, adv: 0.327274, recon: 0.113137, id: 1.137148] time: 0:01:17.124410 \n",
      "[Epoch 0/20] [Batch 274/995] [D loss: 0.274153, acc:  25%] [G loss: 4.593753, adv: 0.415447, recon: 0.130417, id: 0.738379] time: 0:01:17.217045 \n",
      "[Epoch 0/20] [Batch 275/995] [D loss: 0.302506, acc:  18%] [G loss: 5.414997, adv: 0.324529, recon: 0.144776, id: 0.807488] time: 0:01:17.304632 \n",
      "[Epoch 0/20] [Batch 276/995] [D loss: 0.324934, acc:  18%] [G loss: 6.749018, adv: 0.320069, recon: 0.228419, id: 0.724869] time: 0:01:17.487160 \n",
      "[Epoch 0/20] [Batch 277/995] [D loss: 0.304011, acc:  25%] [G loss: 4.744188, adv: 0.436252, recon: 0.120072, id: 0.518155] time: 0:01:17.571023 \n",
      "[Epoch 0/20] [Batch 278/995] [D loss: 0.218487, acc:  56%] [G loss: 5.676996, adv: 0.485513, recon: 0.124425, id: 1.441405] time: 0:01:17.657007 \n",
      "[Epoch 0/20] [Batch 279/995] [D loss: 0.254337, acc:   3%] [G loss: 4.964801, adv: 0.407689, recon: 0.134058, id: 0.679313] time: 0:01:17.739573 \n",
      "[Epoch 0/20] [Batch 280/995] [D loss: 0.315127, acc:   9%] [G loss: 5.279607, adv: 0.349678, recon: 0.138649, id: 1.011858] time: 0:01:17.822694 \n",
      "[Epoch 0/20] [Batch 281/995] [D loss: 0.203844, acc:  31%] [G loss: 5.880012, adv: 0.404059, recon: 0.136104, id: 1.095471] time: 0:01:17.997171 \n",
      "[Epoch 0/20] [Batch 282/995] [D loss: 0.113146, acc:  34%] [G loss: 5.647662, adv: 0.527864, recon: 0.099412, id: 1.335509] time: 0:01:18.083017 \n",
      "[Epoch 0/20] [Batch 283/995] [D loss: 0.618476, acc:  18%] [G loss: 5.252973, adv: 0.299226, recon: 0.108554, id: 1.327706] time: 0:01:18.164753 \n",
      "[Epoch 0/20] [Batch 284/995] [D loss: 0.257315, acc:  25%] [G loss: 4.067778, adv: 0.435496, recon: 0.099832, id: 0.887878] time: 0:01:18.248633 \n",
      "[Epoch 0/20] [Batch 285/995] [D loss: 0.270153, acc:   6%] [G loss: 5.792106, adv: 0.363381, recon: 0.138882, id: 1.332994] time: 0:01:18.334890 \n",
      "[Epoch 0/20] [Batch 286/995] [D loss: 0.273080, acc:  21%] [G loss: 5.883867, adv: 0.384164, recon: 0.135752, id: 1.390992] time: 0:01:18.509363 \n",
      "[Epoch 0/20] [Batch 287/995] [D loss: 0.268705, acc:  31%] [G loss: 5.073909, adv: 0.391140, recon: 0.115756, id: 1.334992] time: 0:01:18.594555 \n",
      "[Epoch 0/20] [Batch 288/995] [D loss: 0.197235, acc:  15%] [G loss: 5.509469, adv: 0.484218, recon: 0.122436, id: 1.556384] time: 0:01:18.679555 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 289/995] [D loss: 0.229492, acc:  18%] [G loss: 5.586353, adv: 0.466579, recon: 0.133972, id: 0.776365] time: 0:01:18.773005 \n",
      "[Epoch 0/20] [Batch 290/995] [D loss: 0.421575, acc:   3%] [G loss: 5.102785, adv: 0.313520, recon: 0.113964, id: 1.193848] time: 0:01:18.847934 \n",
      "[Epoch 0/20] [Batch 291/995] [D loss: 0.131166, acc:  21%] [G loss: 6.242320, adv: 0.508712, recon: 0.142470, id: 1.689168] time: 0:01:19.016279 \n",
      "[Epoch 0/20] [Batch 292/995] [D loss: 0.305646, acc:   3%] [G loss: 5.803340, adv: 0.336483, recon: 0.156038, id: 0.985327] time: 0:01:19.106040 \n",
      "[Epoch 0/20] [Batch 293/995] [D loss: 0.225249, acc:  18%] [G loss: 5.152165, adv: 0.468699, recon: 0.129977, id: 0.875805] time: 0:01:19.190171 \n",
      "[Epoch 0/20] [Batch 294/995] [D loss: 0.398324, acc:  25%] [G loss: 5.283312, adv: 0.334026, recon: 0.151122, id: 0.948904] time: 0:01:19.276275 \n",
      "[Epoch 0/20] [Batch 295/995] [D loss: 0.268161, acc:  28%] [G loss: 4.642446, adv: 0.359493, recon: 0.129394, id: 0.624290] time: 0:01:19.360133 \n",
      "[Epoch 0/20] [Batch 296/995] [D loss: 0.203642, acc:  56%] [G loss: 4.856294, adv: 0.451678, recon: 0.124632, id: 1.166496] time: 0:01:19.532210 \n",
      "[Epoch 0/20] [Batch 297/995] [D loss: 0.320033, acc:  31%] [G loss: 4.425150, adv: 0.391851, recon: 0.125043, id: 0.683202] time: 0:01:19.617779 \n",
      "[Epoch 0/20] [Batch 298/995] [D loss: 0.298460, acc:  21%] [G loss: 4.846046, adv: 0.423161, recon: 0.115032, id: 0.949892] time: 0:01:19.706521 \n",
      "[Epoch 0/20] [Batch 299/995] [D loss: 0.402221, acc:  37%] [G loss: 4.314691, adv: 0.370663, recon: 0.096983, id: 0.868093] time: 0:01:19.795542 \n",
      "[Epoch 0/20] [Batch 300/995] [D loss: 0.277833, acc:   9%] [G loss: 5.247271, adv: 0.480242, recon: 0.155307, id: 0.494636] time: 0:01:19.889025 \n",
      "[Epoch 0/20] [Batch 301/995] [D loss: 0.180624, acc:  12%] [G loss: 6.321277, adv: 0.564282, recon: 0.135082, id: 1.185313] time: 0:01:20.057439 \n",
      "[Epoch 0/20] [Batch 302/995] [D loss: 0.357128, acc:   6%] [G loss: 4.965557, adv: 0.362949, recon: 0.116199, id: 1.545103] time: 0:01:20.143047 \n",
      "[Epoch 0/20] [Batch 303/995] [D loss: 0.244213, acc:  12%] [G loss: 5.121042, adv: 0.443644, recon: 0.151003, id: 0.819581] time: 0:01:20.226399 \n",
      "[Epoch 0/20] [Batch 304/995] [D loss: 0.269153, acc:  21%] [G loss: 6.496437, adv: 0.416209, recon: 0.158404, id: 1.302219] time: 0:01:20.310157 \n",
      "[Epoch 0/20] [Batch 305/995] [D loss: 0.225754, acc:  25%] [G loss: 4.253854, adv: 0.453111, recon: 0.123924, id: 0.629831] time: 0:01:20.399716 \n",
      "[Epoch 0/20] [Batch 306/995] [D loss: 0.201015, acc:  15%] [G loss: 4.126893, adv: 0.514616, recon: 0.099802, id: 0.601950] time: 0:01:20.573666 \n",
      "[Epoch 0/20] [Batch 307/995] [D loss: 0.299773, acc:   9%] [G loss: 5.067484, adv: 0.295817, recon: 0.142070, id: 0.875794] time: 0:01:20.651091 \n",
      "[Epoch 0/20] [Batch 308/995] [D loss: 0.333133, acc:  21%] [G loss: 4.235981, adv: 0.409673, recon: 0.098589, id: 0.977115] time: 0:01:20.734547 \n",
      "[Epoch 0/20] [Batch 309/995] [D loss: 0.326261, acc:  31%] [G loss: 4.479262, adv: 0.387771, recon: 0.113904, id: 0.851553] time: 0:01:20.818173 \n",
      "[Epoch 0/20] [Batch 310/995] [D loss: 0.253500, acc:  40%] [G loss: 5.335831, adv: 0.406309, recon: 0.141968, id: 0.848226] time: 0:01:20.901594 \n",
      "[Epoch 0/20] [Batch 311/995] [D loss: 0.302885, acc:  40%] [G loss: 5.103179, adv: 0.416375, recon: 0.102463, id: 1.377957] time: 0:01:21.083127 \n",
      "[Epoch 0/20] [Batch 312/995] [D loss: 0.217093, acc:  31%] [G loss: 4.620935, adv: 0.458237, recon: 0.103729, id: 1.132454] time: 0:01:21.211582 \n",
      "[Epoch 0/20] [Batch 313/995] [D loss: 0.309609, acc:   6%] [G loss: 5.998871, adv: 0.394528, recon: 0.135394, id: 1.712679] time: 0:01:21.312906 \n",
      "[Epoch 0/20] [Batch 314/995] [D loss: 0.323077, acc:   0%] [G loss: 4.625391, adv: 0.268524, recon: 0.122672, id: 0.751115] time: 0:01:21.396799 \n",
      "[Epoch 0/20] [Batch 315/995] [D loss: 0.225923, acc:  12%] [G loss: 5.190367, adv: 0.384027, recon: 0.123823, id: 1.075210] time: 0:01:21.487558 \n",
      "[Epoch 0/20] [Batch 316/995] [D loss: 0.212135, acc:  37%] [G loss: 5.876730, adv: 0.491058, recon: 0.126083, id: 1.090580] time: 0:01:21.654876 \n",
      "[Epoch 0/20] [Batch 317/995] [D loss: 0.286027, acc:   0%] [G loss: 4.639143, adv: 0.285535, recon: 0.117732, id: 0.989331] time: 0:01:21.738967 \n",
      "[Epoch 0/20] [Batch 318/995] [D loss: 0.273630, acc:  21%] [G loss: 4.851852, adv: 0.435221, recon: 0.121991, id: 1.142800] time: 0:01:21.822952 \n",
      "[Epoch 0/20] [Batch 319/995] [D loss: 0.396039, acc:  31%] [G loss: 4.684705, adv: 0.352549, recon: 0.125995, id: 0.592655] time: 0:01:21.911862 \n",
      "[Epoch 0/20] [Batch 320/995] [D loss: 0.271687, acc:  25%] [G loss: 4.534475, adv: 0.422842, recon: 0.124046, id: 0.702686] time: 0:01:21.998632 \n",
      "[Epoch 0/20] [Batch 321/995] [D loss: 0.215208, acc:  37%] [G loss: 4.975737, adv: 0.432897, recon: 0.139185, id: 0.894546] time: 0:01:22.171070 \n",
      "[Epoch 0/20] [Batch 322/995] [D loss: 0.276713, acc:  34%] [G loss: 4.454299, adv: 0.518162, recon: 0.113700, id: 0.813590] time: 0:01:22.258806 \n",
      "[Epoch 0/20] [Batch 323/995] [D loss: 0.217030, acc:   6%] [G loss: 4.058114, adv: 0.365147, recon: 0.108428, id: 0.785310] time: 0:01:22.342369 \n",
      "[Epoch 0/20] [Batch 324/995] [D loss: 0.201120, acc:  12%] [G loss: 4.295981, adv: 0.372662, recon: 0.126417, id: 0.632884] time: 0:01:22.430025 \n",
      "[Epoch 0/20] [Batch 325/995] [D loss: 0.197353, acc:  28%] [G loss: 5.893526, adv: 0.357010, recon: 0.172617, id: 1.298139] time: 0:01:22.513982 \n",
      "[Epoch 0/20] [Batch 326/995] [D loss: 0.317412, acc:  28%] [G loss: 4.351677, adv: 0.378778, recon: 0.118243, id: 0.505556] time: 0:01:22.684103 \n",
      "[Epoch 0/20] [Batch 327/995] [D loss: 0.224277, acc:  31%] [G loss: 5.712225, adv: 0.381249, recon: 0.143058, id: 1.434776] time: 0:01:22.767858 \n",
      "[Epoch 0/20] [Batch 328/995] [D loss: 0.173641, acc:  25%] [G loss: 6.397461, adv: 0.434177, recon: 0.158448, id: 1.231409] time: 0:01:22.854588 \n",
      "[Epoch 0/20] [Batch 329/995] [D loss: 0.361572, acc:  25%] [G loss: 5.303987, adv: 0.331273, recon: 0.141043, id: 0.788606] time: 0:01:22.938265 \n",
      "[Epoch 0/20] [Batch 330/995] [D loss: 0.266695, acc:  28%] [G loss: 5.734516, adv: 0.431114, recon: 0.153413, id: 1.099820] time: 0:01:23.037600 \n",
      "[Epoch 0/20] [Batch 331/995] [D loss: 0.305970, acc:  15%] [G loss: 4.646946, adv: 0.362932, recon: 0.113149, id: 0.790527] time: 0:01:23.208294 \n",
      "[Epoch 0/20] [Batch 332/995] [D loss: 0.348773, acc:  40%] [G loss: 4.451756, adv: 0.357088, recon: 0.131098, id: 0.597890] time: 0:01:23.292633 \n",
      "[Epoch 0/20] [Batch 333/995] [D loss: 0.200909, acc:  21%] [G loss: 3.895095, adv: 0.480534, recon: 0.079102, id: 1.112469] time: 0:01:23.384851 \n",
      "[Epoch 0/20] [Batch 334/995] [D loss: 0.369758, acc:   0%] [G loss: 6.924986, adv: 0.280253, recon: 0.213372, id: 1.294200] time: 0:01:23.468409 \n",
      "[Epoch 0/20] [Batch 335/995] [D loss: 0.277552, acc:  21%] [G loss: 4.926059, adv: 0.404693, recon: 0.136101, id: 0.734685] time: 0:01:23.558775 \n",
      "[Epoch 0/20] [Batch 336/995] [D loss: 0.370513, acc:  12%] [G loss: 5.833062, adv: 0.290569, recon: 0.151075, id: 0.956706] time: 0:01:23.734171 \n",
      "[Epoch 0/20] [Batch 337/995] [D loss: 0.298018, acc:  28%] [G loss: 4.579051, adv: 0.457911, recon: 0.123332, id: 0.664723] time: 0:01:23.816339 \n",
      "[Epoch 0/20] [Batch 338/995] [D loss: 0.190964, acc:  53%] [G loss: 4.848633, adv: 0.514204, recon: 0.112926, id: 1.250079] time: 0:01:23.901349 \n",
      "[Epoch 0/20] [Batch 339/995] [D loss: 0.292403, acc:  28%] [G loss: 5.837922, adv: 0.373203, recon: 0.133392, id: 1.624124] time: 0:01:23.999739 \n",
      "[Epoch 0/20] [Batch 340/995] [D loss: 0.323248, acc:   0%] [G loss: 5.051985, adv: 0.375715, recon: 0.130337, id: 0.616274] time: 0:01:24.084992 \n",
      "[Epoch 0/20] [Batch 341/995] [D loss: 0.244658, acc:  21%] [G loss: 5.757207, adv: 0.362435, recon: 0.126431, id: 1.161260] time: 0:01:24.260119 \n",
      "[Epoch 0/20] [Batch 342/995] [D loss: 0.307735, acc:  28%] [G loss: 5.685482, adv: 0.294414, recon: 0.141644, id: 1.195226] time: 0:01:24.343433 \n",
      "[Epoch 0/20] [Batch 343/995] [D loss: 0.194453, acc:   9%] [G loss: 4.929609, adv: 0.407902, recon: 0.111212, id: 1.348164] time: 0:01:24.427163 \n",
      "[Epoch 0/20] [Batch 344/995] [D loss: 0.213483, acc:  12%] [G loss: 5.128669, adv: 0.397872, recon: 0.126643, id: 0.876510] time: 0:01:24.510490 \n",
      "[Epoch 0/20] [Batch 345/995] [D loss: 0.348066, acc:   6%] [G loss: 4.768633, adv: 0.291006, recon: 0.125464, id: 1.091712] time: 0:01:24.594479 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 346/995] [D loss: 0.298574, acc:  40%] [G loss: 4.820464, adv: 0.388820, recon: 0.089262, id: 1.193200] time: 0:01:24.765742 \n",
      "[Epoch 0/20] [Batch 347/995] [D loss: 0.253737, acc:   9%] [G loss: 4.546799, adv: 0.398378, recon: 0.109254, id: 1.181381] time: 0:01:24.860804 \n",
      "[Epoch 0/20] [Batch 348/995] [D loss: 0.240115, acc:  31%] [G loss: 4.756771, adv: 0.383646, recon: 0.121950, id: 0.939813] time: 0:01:24.944361 \n",
      "[Epoch 0/20] [Batch 349/995] [D loss: 0.258652, acc:  21%] [G loss: 4.691925, adv: 0.293111, recon: 0.113840, id: 1.247163] time: 0:01:25.028227 \n",
      "[Epoch 0/20] [Batch 350/995] [D loss: 0.245525, acc:  18%] [G loss: 5.360309, adv: 0.316676, recon: 0.133332, id: 1.131468] time: 0:01:25.116687 \n",
      "[Epoch 0/20] [Batch 351/995] [D loss: 0.268118, acc:  15%] [G loss: 4.793578, adv: 0.289182, recon: 0.141361, id: 0.938020] time: 0:01:25.288861 \n",
      "[Epoch 0/20] [Batch 352/995] [D loss: 0.248973, acc:  21%] [G loss: 4.898986, adv: 0.396548, recon: 0.131007, id: 1.009476] time: 0:01:25.379166 \n",
      "[Epoch 0/20] [Batch 353/995] [D loss: 0.258988, acc:  25%] [G loss: 4.723761, adv: 0.374793, recon: 0.113299, id: 1.103328] time: 0:01:25.462931 \n",
      "[Epoch 0/20] [Batch 354/995] [D loss: 0.255564, acc:   6%] [G loss: 5.805158, adv: 0.412261, recon: 0.156204, id: 0.683227] time: 0:01:25.547142 \n",
      "[Epoch 0/20] [Batch 355/995] [D loss: 0.211079, acc:  28%] [G loss: 5.243841, adv: 0.372750, recon: 0.118074, id: 1.126678] time: 0:01:25.631451 \n",
      "[Epoch 0/20] [Batch 356/995] [D loss: 0.274327, acc:  15%] [G loss: 4.334683, adv: 0.322519, recon: 0.119938, id: 0.826457] time: 0:01:25.816737 \n",
      "[Epoch 0/20] [Batch 357/995] [D loss: 0.294390, acc:  28%] [G loss: 5.030932, adv: 0.323086, recon: 0.100391, id: 1.135431] time: 0:01:25.900261 \n",
      "[Epoch 0/20] [Batch 358/995] [D loss: 0.273479, acc:  18%] [G loss: 4.485774, adv: 0.367230, recon: 0.099678, id: 1.008985] time: 0:01:25.983785 \n",
      "[Epoch 0/20] [Batch 359/995] [D loss: 0.253275, acc:  31%] [G loss: 4.746509, adv: 0.367136, recon: 0.120786, id: 1.006378] time: 0:01:26.067540 \n",
      "[Epoch 0/20] [Batch 360/995] [D loss: 0.150770, acc:  12%] [G loss: 4.996120, adv: 0.424949, recon: 0.127571, id: 1.028887] time: 0:01:26.151486 \n",
      "[Epoch 0/20] [Batch 361/995] [D loss: 0.241379, acc:   9%] [G loss: 5.114528, adv: 0.458668, recon: 0.134083, id: 0.928914] time: 0:01:26.367318 \n",
      "[Epoch 0/20] [Batch 362/995] [D loss: 0.160251, acc:  12%] [G loss: 4.465002, adv: 0.380870, recon: 0.133615, id: 0.747830] time: 0:01:26.452377 \n",
      "[Epoch 0/20] [Batch 363/995] [D loss: 0.375714, acc:  18%] [G loss: 4.108270, adv: 0.304887, recon: 0.118356, id: 0.753515] time: 0:01:26.536624 \n",
      "[Epoch 0/20] [Batch 364/995] [D loss: 0.322290, acc:  28%] [G loss: 4.724819, adv: 0.340885, recon: 0.105855, id: 1.239709] time: 0:01:26.621066 \n",
      "[Epoch 0/20] [Batch 365/995] [D loss: 0.239009, acc:  34%] [G loss: 4.692159, adv: 0.399084, recon: 0.132588, id: 0.587788] time: 0:01:26.714052 \n",
      "[Epoch 0/20] [Batch 366/995] [D loss: 0.226317, acc:  15%] [G loss: 5.003309, adv: 0.438150, recon: 0.126714, id: 1.047433] time: 0:01:26.889163 \n",
      "[Epoch 0/20] [Batch 367/995] [D loss: 0.165934, acc:  21%] [G loss: 4.557496, adv: 0.452765, recon: 0.096912, id: 1.330757] time: 0:01:26.968682 \n",
      "[Epoch 0/20] [Batch 368/995] [D loss: 0.345769, acc:  37%] [G loss: 6.042134, adv: 0.310579, recon: 0.156560, id: 1.370950] time: 0:01:27.053075 \n",
      "[Epoch 0/20] [Batch 369/995] [D loss: 0.237947, acc:   0%] [G loss: 4.674950, adv: 0.446695, recon: 0.098113, id: 0.661061] time: 0:01:27.138029 \n",
      "[Epoch 0/20] [Batch 370/995] [D loss: 0.360440, acc:  25%] [G loss: 4.951347, adv: 0.288326, recon: 0.138895, id: 1.029503] time: 0:01:27.222356 \n",
      "[Epoch 0/20] [Batch 371/995] [D loss: 0.334517, acc:  25%] [G loss: 5.657210, adv: 0.336393, recon: 0.129338, id: 1.232680] time: 0:01:27.409259 \n",
      "[Epoch 0/20] [Batch 372/995] [D loss: 0.232638, acc:   3%] [G loss: 6.089887, adv: 0.513681, recon: 0.178233, id: 0.937891] time: 0:01:27.493252 \n",
      "[Epoch 0/20] [Batch 373/995] [D loss: 0.227538, acc:   9%] [G loss: 5.848463, adv: 0.431360, recon: 0.150627, id: 1.160031] time: 0:01:27.582250 \n",
      "[Epoch 0/20] [Batch 374/995] [D loss: 0.301480, acc:  18%] [G loss: 3.497447, adv: 0.355033, recon: 0.090329, id: 0.636330] time: 0:01:27.665540 \n",
      "[Epoch 0/20] [Batch 375/995] [D loss: 0.240247, acc:   9%] [G loss: 5.870938, adv: 0.368533, recon: 0.159952, id: 0.662933] time: 0:01:27.753708 \n",
      "[Epoch 0/20] [Batch 376/995] [D loss: 0.337618, acc:  12%] [G loss: 4.387949, adv: 0.270812, recon: 0.123922, id: 0.759157] time: 0:01:27.932744 \n",
      "[Epoch 0/20] [Batch 377/995] [D loss: 0.176241, acc:  15%] [G loss: 3.491160, adv: 0.394562, recon: 0.076717, id: 0.918597] time: 0:01:28.029146 \n",
      "[Epoch 0/20] [Batch 378/995] [D loss: 0.385252, acc:  25%] [G loss: 6.156489, adv: 0.321694, recon: 0.137428, id: 1.640086] time: 0:01:28.114458 \n",
      "[Epoch 0/20] [Batch 379/995] [D loss: 0.270642, acc:  12%] [G loss: 5.560650, adv: 0.431114, recon: 0.135996, id: 1.298499] time: 0:01:28.198543 \n",
      "[Epoch 0/20] [Batch 380/995] [D loss: 0.339928, acc:  34%] [G loss: 4.566951, adv: 0.342890, recon: 0.094531, id: 0.938060] time: 0:01:28.285677 \n",
      "[Epoch 0/20] [Batch 381/995] [D loss: 0.246982, acc:  15%] [G loss: 4.751031, adv: 0.347747, recon: 0.106758, id: 1.359785] time: 0:01:28.457180 \n",
      "[Epoch 0/20] [Batch 382/995] [D loss: 0.215477, acc:   0%] [G loss: 5.376919, adv: 0.415169, recon: 0.129903, id: 1.146665] time: 0:01:28.537630 \n",
      "[Epoch 0/20] [Batch 383/995] [D loss: 0.214872, acc:  15%] [G loss: 3.739689, adv: 0.375255, recon: 0.078598, id: 1.046165] time: 0:01:28.629739 \n",
      "[Epoch 0/20] [Batch 384/995] [D loss: 0.353850, acc:  37%] [G loss: 5.022267, adv: 0.309177, recon: 0.139383, id: 0.732007] time: 0:01:28.714641 \n",
      "[Epoch 0/20] [Batch 385/995] [D loss: 0.204598, acc:  28%] [G loss: 4.347953, adv: 0.414743, recon: 0.115459, id: 0.811646] time: 0:01:28.798728 \n",
      "[Epoch 0/20] [Batch 386/995] [D loss: 0.472530, acc:   9%] [G loss: 4.589863, adv: 0.287218, recon: 0.123160, id: 0.354074] time: 0:01:28.969852 \n",
      "[Epoch 0/20] [Batch 387/995] [D loss: 0.262811, acc:  25%] [G loss: 4.755468, adv: 0.411799, recon: 0.121409, id: 0.837916] time: 0:01:29.054549 \n",
      "[Epoch 0/20] [Batch 388/995] [D loss: 0.192407, acc:  28%] [G loss: 5.006811, adv: 0.400903, recon: 0.127417, id: 0.839832] time: 0:01:29.149409 \n",
      "[Epoch 0/20] [Batch 389/995] [D loss: 0.262120, acc:  25%] [G loss: 4.714283, adv: 0.543094, recon: 0.083337, id: 1.600801] time: 0:01:29.241099 \n",
      "[Epoch 0/20] [Batch 390/995] [D loss: 0.264910, acc:   3%] [G loss: 4.751441, adv: 0.337231, recon: 0.134721, id: 0.795357] time: 0:01:29.325090 \n",
      "[Epoch 0/20] [Batch 391/995] [D loss: 0.191657, acc:  21%] [G loss: 6.118583, adv: 0.444540, recon: 0.124084, id: 1.147769] time: 0:01:29.507775 \n",
      "[Epoch 0/20] [Batch 392/995] [D loss: 0.270264, acc:  15%] [G loss: 4.608604, adv: 0.416364, recon: 0.123596, id: 0.668292] time: 0:01:29.595994 \n",
      "[Epoch 0/20] [Batch 393/995] [D loss: 0.181061, acc:   0%] [G loss: 4.556576, adv: 0.454579, recon: 0.092879, id: 1.268262] time: 0:01:29.676099 \n",
      "[Epoch 0/20] [Batch 394/995] [D loss: 0.344260, acc:  21%] [G loss: 4.276893, adv: 0.300092, recon: 0.108484, id: 1.263988] time: 0:01:29.760386 \n",
      "[Epoch 0/20] [Batch 395/995] [D loss: 0.277101, acc:   0%] [G loss: 4.067291, adv: 0.326579, recon: 0.107666, id: 0.710994] time: 0:01:29.849954 \n",
      "[Epoch 0/20] [Batch 396/995] [D loss: 0.496097, acc:  18%] [G loss: 4.388645, adv: 0.363515, recon: 0.084935, id: 1.518860] time: 0:01:30.028118 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bcff29017444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCycleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c8954d7e550d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;31m# Translate images to opposite domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_AB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mfake_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_BA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;31m#print(fake_A.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;31m#print(fake_B.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py3keras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/miniconda2/envs/py3keras/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py3keras/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py3keras/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = CycleGAN()\n",
    "    gan.train(epochs=20, batch_size=1, sample_interval=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theano'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.lib.utils._Deprecate.__call__.<locals>.newfunc(*args, **kwds)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.misc.imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose images to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
